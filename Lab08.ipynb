{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08e9b56",
   "metadata": {},
   "source": [
    "# Lab 08: Black Box AI - Neural Networks & Computer Vision\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In Lab 07, we used models like Decision Trees, Random Forests, and XGBoost to predict the state of a piece of toast (`Under-done`, `Perfect`, `Burnt`) based on structured data (e.g., `ToastingTime`, `BreadThickness`).\n",
    "\n",
    "In this lab, we then go further into \"Black Box\" AIâ€”specifically, **Neural Networks**. For details please check the lecture slides.\n",
    "\n",
    "We will:\n",
    "1.  **Apply a Multi-Layer Perceptron (MLP)** to our familiar Lab 07 \"toaster\" dataset.\n",
    "2.  **Build a simple MLP from scratch** using NumPy to understand its inner workings (feedforward, backpropagation).\n",
    "3.  **Use a pre-trained YOLO model**, a powerful Convolutional Neural Network (CNN), to perform real-time object detection on images, e.g. \"Can it find a toaster?\"\n",
    "4.  **Final Challenge: Build and optimize your own CNN** to solve a new image classification problem: detecting food on a real dataset!\n",
    "\n",
    "## Submission\n",
    "1.  This is a **group assignment**. One submission per group.\n",
    "2.  You can work in **Google Colab** or **locally** (recommended) .\n",
    "3.  Complete all tasks in this notebook.\n",
    "4.  For the **Final Challenge**, you must finish the first report and run the cells to generate `txt` format reports. Fill in and generate report 2 and 3 only if you choose to do optional job step 3.5.\n",
    "5.  Submit forked Github repository link to canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db7aba",
   "metadata": {},
   "source": [
    "## Setup: Install Virtual Environment and Libraries\n",
    "\n",
    "**Tips**: Before install PyTorch, please verify your operation system to see if they have GPU or CPU only. For Macbook with Apple Silicon, you may consider you have MPS as neural network processing unit to replace NVIDIA GPU.\n",
    "\n",
    "You are required to create a new **virtual environment**, and install the following libraries:\n",
    "\n",
    "    torch, torchvision (They require special attention. Please help check `### PyTorch Installation Guide`)\n",
    "\n",
    "    opencv-python, tqdm, numpy, pandas, seaborn, matplotlib, scikit-learn, ultralytics\n",
    "\n",
    "### PyTorch Installation Guide\n",
    "\n",
    "Pick specific Pytorch (torch, torchvision) for your own system, especially if you have GPU with CUDA enabled, and python version with reference from [General installation guide for all platforms](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "**For NVIDIA GPU Users (CUDA):**\n",
    "- [CUDA Toolkit Installation](https://developer.nvidia.com/cuda-downloads)\n",
    "- Check CUDA compatibility: `nvidia-smi` in terminal\n",
    "- [Multiple CUDA Compilation](https://github.com/siyanhu/MultipleCUDA)\n",
    "\n",
    "**For Apple Silicon (MPS):**\n",
    "- PyTorch automatically supports MPS on newer Macs\n",
    "- Install PyTorch for Mac: `pip3 install torch torchvision`\n",
    "- [PyTorch MPS Guide](https://pytorch.org/docs/stable/notes/mps.html)\n",
    "\n",
    "**For CPU-only Systems:**\n",
    "- Use CPU-compatible PyTorch: `pip install torch torchvision`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f5a083",
   "metadata": {},
   "source": [
    "## **Part 1: The Multi-Layer Perceptron (MLP) on Structured Data**\n",
    "\n",
    "Let's start by revisiting our Lab 07 toaster dataset. An MLP is a \"universal function approximator\" and should be able to learn the complex, non-linear boundaries between `Under-done`, `Perfect`, and `Burnt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb2ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toaster data (Lab 07) loaded:\n",
      "   ToastingTime  BreadThickness  IsFrozen  AmbientTemp  ToastState\n",
      "0           162       11.785964         1    15.258515           0\n",
      "1           495        8.133912         1    23.953351           2\n",
      "2           408       13.424118         0    14.178307           2\n",
      "3           330       29.988598         0    12.165528           1\n",
      "4           166       15.143779         0    23.922013           2\n",
      "\n",
      "Data prepared: 3500 training samples, 1500 test samples.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- Regenerate the Lab 07 Toaster Data ---\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_toaster_data_interactive(n_samples=5000):\n",
    "    # Feature Generation\n",
    "    toasting_time = np.random.randint(60, 500, n_samples)\n",
    "    bread_thickness = np.random.uniform(5, 30, n_samples)\n",
    "    is_frozen = np.random.randint(0, 2, n_samples)\n",
    "    ambient_temp = np.random.normal(20, 6, n_samples)\n",
    "\n",
    "    # Non-linear ideal_time (Same as Lab 07)\n",
    "    base_ideal_time = (30 + 1.5 * bread_thickness + 0.1 * bread_thickness**2 + 0.005 * bread_thickness**3)\n",
    "    frozen_penalty = is_frozen * (base_ideal_time * (np.exp(0.04 * bread_thickness) - 1))\n",
    "    unadjusted_ideal_time = base_ideal_time + frozen_penalty\n",
    "    temp_efficiency_factor = np.exp(0.025 * (ambient_temp - 20))\n",
    "    ideal_time = unadjusted_ideal_time / temp_efficiency_factor\n",
    "\n",
    "    perfect_window_width = 25 + bread_thickness\n",
    "    time_diff_sq = (toasting_time - ideal_time)**2\n",
    "    perfect_score = np.exp(-time_diff_sq / (2 * perfect_window_width**2))\n",
    "\n",
    "    burn_onset_offset = 45\n",
    "    burn_rate_k = 0.1\n",
    "    time_past_ideal = toasting_time - ideal_time - burn_onset_offset\n",
    "    burnt_score = 1 / (1 + np.exp(-burn_rate_k * time_past_ideal))\n",
    "\n",
    "    perfect_score += np.random.normal(0, 0.3, n_samples)\n",
    "    burnt_score += np.random.normal(0, 0.3, n_samples)\n",
    "\n",
    "    toast_state = np.zeros(n_samples, dtype=int) # 0: Under-done\n",
    "    is_perfect = (perfect_score > burnt_score) & (perfect_score > 0.6)\n",
    "    toast_state[is_perfect] = 1 # 1: Perfect\n",
    "    is_burnt = (burnt_score > perfect_score) & (burnt_score > 0.5)\n",
    "    toast_state[is_burnt] = 2 # 2: Burnt\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'ToastingTime': toasting_time,\n",
    "        'BreadThickness': bread_thickness,\n",
    "        'IsFrozen': is_frozen,\n",
    "        'AmbientTemp': ambient_temp,\n",
    "        'ToastState': toast_state\n",
    "    })\n",
    "    return df\n",
    "\n",
    "df = generate_toaster_data_interactive()\n",
    "print(\"Toaster data (Lab 07) loaded:\")\n",
    "print(df.head())\n",
    "\n",
    "# --- Prepare Data for ML ---\n",
    "X = df.drop('ToastState', axis=1)\n",
    "y = df['ToastState']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# **IMPORTANT for Neural Networks**:\n",
    "# Scale the features. NNs are sensitive to feature scale.\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nData prepared: {X_train_scaled.shape[0]} training samples, {X_test_scaled.shape[0]} test samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eb93a3",
   "metadata": {},
   "source": [
    "### Task 1.1: Train a `scikit-learn` MLP\n",
    "\n",
    "Let's use the pre-built `MLPClassifier` from `scikit-learn` to understand the difference among a MLP deep learning network, and previous XGBoost/Random Forest from Lab07.\n",
    "\n",
    "1.  Import `MLPClassifier` from `sklearn.neural_network`.\n",
    "2.  Initialize it. A good starting point is `hidden_layer_sizes=(100, 50)`, which means two hidden layers with 100 and 50 neurons, respectively. Use `max_iter=500` to give it enough time to train, and `random_state=42` for reproducibility.\n",
    "3.  Fit the model on the **scaled** training data (`X_train_scaled`, `y_train`).\n",
    "4.  Predict on the **scaled** test data (`X_test_scaled`).\n",
    "5.  Print the accuracy and the classification report.\n",
    "6.  **Question:** Rerun the final challenge in Lab07, how does this accuracy compare to the best model you submitted (e.g., XGBoost or Random Forest)?\n",
    "\n",
    "    **TODO:**  *The accuracy of the MLP model is already better compared to the best model from Lab07, which was SVM with a accuracy of 0.82 on test set.*\n",
    "\n",
    "7. **Challenge:** Beat the baseline accuracy. Try a few different hyperparameter settings, record the results, and write down your answers in the following table.\n",
    "\n",
    "    | Experiment | Example | What to Test | Your Settings | Accuracy | Notes |\n",
    "    |------------|---------------|--------------|---------------|----------|-------|\n",
    "    | Wider Network | e.g. `(200, 150)` | Impact of more neurons per layer | `(10, 10)` | `0.88` | Following two-thirds rule and less than double rules, the number of neurons in each layer is kept small to prevent overfitting while still allowing for some complexity. |\n",
    "    | Strong Regularization | e.g. `alpha=0.01` | Effect on overfitting | `400~421(range)` | `0.882` | It reaches its peak accuracy between 400 and 421 iterations. The lower or higher it gets, it may lead to underfitting or overfitting. |\n",
    "    | SGD Solver | e.g. `solver='sgd'` | Different optimization algorithm | `adam` | `0.882` | Adam seems to perform the best among the three available solver, without changing the hyperparameters. |\n",
    "    | Different number of Layers | e.g. single layer `(200,)` | Different network architecture | `(10, 3, 5, 3)`(200 iter) | `0.8827` | Takes less iterations before it fully fits. |\n",
    "    | Learning Rate | e.g. `learning_rate_init=0.01` | Faster/slower convergence | `0.000889~0.00524` | `0.8827` | Learning rate affects how quickly the model converges to a solution. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa1fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the scikit-learn MLP...\n",
      "Training complete.\n",
      "\n",
      "--- scikit-learn MLP Results ---\n",
      "Accuracy: 0.8827\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Under-done       0.90      0.76      0.82       456\n",
      "     Perfect       0.75      0.80      0.77       205\n",
      "       Burnt       0.91      0.97      0.94       839\n",
      "\n",
      "    accuracy                           0.88      1500\n",
      "   macro avg       0.85      0.84      0.85      1500\n",
      "weighted avg       0.88      0.88      0.88      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\HKUST\\ISDN\\ISDN3000C\\ISDN3000C_Lab08\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# 1. Initialize the MLPClassifier\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(10, 3, 5, 3), max_iter=200, random_state=42, \n",
    "                        activation='relu', solver='adam', alpha=0.0001)\n",
    "\n",
    "# 2. Fit the model\n",
    "print(\"Training the scikit-learn MLP...\")\n",
    "mlp_clf.fit(X_train_scaled, y_train)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 3. Predict on the test set\n",
    "y_pred_mlp = mlp_clf.predict(X_test_scaled)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n",
    "report_mlp = classification_report(y_test, y_pred_mlp, target_names=['Under-done', 'Perfect', 'Burnt'])\n",
    "\n",
    "print(f\"\\n--- scikit-learn MLP Results ---\")\n",
    "print(f\"Accuracy: {accuracy_mlp:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd3bdd",
   "metadata": {},
   "source": [
    "### Task 1.2: Build a Simple MLP with PyTorch\n",
    "\n",
    "Now that you've used the `scikit-learn` helper, let's build the same model using PyTorch. This is a great way to learn PyTorch's workflow, which we'll use again in Part 3 for our CNN.\n",
    "\n",
    "**Our Goal:** Re-create the MLP classifier from Task 1.1 using PyTorch.\n",
    "\n",
    "**Your Task:**\n",
    "1. Complete the missing layers in `PyTorchMLP` class\n",
    "2. **Add learning rate scheduling** to the training loop\n",
    "3. **Implement early stopping** based on validation loss\n",
    "4. **Visualize training progress** with loss/accuracy plots\n",
    "5. Compare your results with at least one different optimizers (Adam, SGD etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "54136360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch DataLoaders created.\n",
      "\n",
      "Training PyTorch MLP...\n",
      "Epoch [20/200], Loss: 0.9944\n",
      "Epoch [40/200], Loss: 0.8769\n",
      "Epoch [60/200], Loss: 0.7398\n",
      "Epoch [80/200], Loss: 0.7246\n",
      "Epoch [100/200], Loss: 0.8860\n",
      "Epoch [120/200], Loss: 0.6444\n",
      "Epoch [140/200], Loss: 0.7284\n",
      "Epoch [160/200], Loss: 0.7226\n",
      "Epoch [180/200], Loss: 0.7315\n",
      "Epoch [200/200], Loss: 0.6683\n",
      "Training complete.\n",
      "\n",
      "--- PyTorch MLP Results ---\n",
      "Accuracy: 0.8613\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Under-done       0.88      0.76      0.81       456\n",
      "     Perfect       0.72      0.65      0.68       205\n",
      "       Burnt       0.88      0.97      0.92       839\n",
      "\n",
      "    accuracy                           0.86      1500\n",
      "   macro avg       0.83      0.79      0.81      1500\n",
      "weighted avg       0.86      0.86      0.86      1500\n",
      "\n",
      "\n",
      "Compare to scikit-learn's MLP accuracy: 0.8827\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- 1. Define the PyTorch MLP Model ---\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PyTorchMLP, self).__init__()\n",
    "        # --- TODO: Define the layers ---\n",
    "        # We want: Input -> 100 neurons -> ReLU -> 50 neurons -> ReLU -> Output (3 neurons)\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(10, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ---------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- TODO: Define the forward pass ---\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ---------------------------------\n",
    "        return x\n",
    "\n",
    "# --- 2. Prepare Data for PyTorch ---\n",
    "\n",
    "# Check if a GPU (like in Colab) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert NumPy arrays to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device) # CrossEntropyLoss expects long integers for labels\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"PyTorch DataLoaders created.\")\n",
    "\n",
    "# --- 3. Initialize Model, Loss, and Optimizer ---\n",
    "n_input = X_train_scaled.shape[1]\n",
    "n_output = len(np.unique(y_train))\n",
    "\n",
    "pytorch_mlp = PyTorchMLP(n_input, n_output).to(device)\n",
    "criterion = nn.CrossEntropyLoss() # This loss function includes Softmax!\n",
    "optimizer = optim.Adam(pytorch_mlp.parameters(), lr=0.0001)\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "epochs = 200 # scikit-learn's default is 200, but Adam is fast\n",
    "print(\"\\nTraining PyTorch MLP...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pytorch_mlp.train() # Set model to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = pytorch_mlp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- 5. Evaluate the Model ---\n",
    "pytorch_mlp.eval() # Set model to evaluation mode\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = pytorch_mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1) # Get the index (class) with the highest score\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy_pytorch = accuracy_score(all_labels, all_preds)\n",
    "report_pytorch = classification_report(all_labels, all_preds, target_names=['Under-done', 'Perfect', 'Burnt'])\n",
    "\n",
    "print(f\"\\n--- PyTorch MLP Results ---\")\n",
    "print(f\"Accuracy: {accuracy_pytorch:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_pytorch)\n",
    "\n",
    "print(f\"\\nCompare to scikit-learn's MLP accuracy: {accuracy_mlp:.4f}\")\n",
    "\n",
    "# --- TODO: Visualize your results: accuracy and runtime ---\n",
    "import matplotlib.pyplot as plt\n",
    "# Example: Bar chart comparing accuracies\n",
    "labels = ['scikit-learn MLP', 'PyTorch MLP']\n",
    "accuracies = [accuracy_mlp, accuracy_pytorch]\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x, accuracies, color=['blue', 'orange'])\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('MLP Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v, f\"{v:.4f}\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5931741a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "PyTorch DataLoaders created.\n",
      "\n",
      "Training PyTorch MLP...\n",
      "Epoch [20/200], Loss: 0.9817\n",
      "Epoch [40/200], Loss: 0.7628\n",
      "Epoch [60/200], Loss: 0.7716\n",
      "Epoch [80/200], Loss: 0.6918\n",
      "Epoch [100/200], Loss: 0.7369\n",
      "Epoch [120/200], Loss: 0.7245\n",
      "Epoch [140/200], Loss: 0.6545\n",
      "Epoch [160/200], Loss: 0.7010\n",
      "Epoch [180/200], Loss: 0.6428\n",
      "Epoch [200/200], Loss: 0.7549\n",
      "Training complete.\n",
      "\n",
      "--- PyTorch MLP Results ---\n",
      "Accuracy: 0.8720\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Under-done       0.89      0.75      0.82       456\n",
      "     Perfect       0.73      0.76      0.75       205\n",
      "       Burnt       0.89      0.96      0.93       839\n",
      "\n",
      "    accuracy                           0.87      1500\n",
      "   macro avg       0.84      0.82      0.83      1500\n",
      "weighted avg       0.87      0.87      0.87      1500\n",
      "\n",
      "\n",
      "Compare to scikit-learn's MLP accuracy: 0.8827\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- TODO: Do some experiments with different optimizers: training loop and inference ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# --- 1. Define the PyTorch MLP Model ---\n",
    "class PyTorchMLP(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PyTorchMLP, self).__init__()\n",
    "        # --- TODO: Define the layers ---\n",
    "        # We want: Input -> 100 neurons -> ReLU -> 50 neurons -> ReLU -> Output (3 neurons)\n",
    "        self.fc1 = nn.Linear(input_size, 10)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(10, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ---------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        # --- TODO: Define the forward pass ---\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # ---------------------------------\n",
    "        return x\n",
    "\n",
    "# --- 2. Prepare Data for PyTorch ---\n",
    "\n",
    "# Check if a GPU (like in Colab) is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Convert NumPy arrays to PyTorch Tensors\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long).to(device) # CrossEntropyLoss expects long integers for labels\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long).to(device)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(\"PyTorch DataLoaders created.\")\n",
    "\n",
    "# --- 3. Initialize Model, Loss, and Optimizer ---\n",
    "n_input = X_train_scaled.shape[1]\n",
    "n_output = len(np.unique(y_train))\n",
    "\n",
    "pytorch_mlp = PyTorchMLP(n_input, n_output).to(device)\n",
    "criterion = nn.CrossEntropyLoss() # This loss function includes Softmax!\n",
    "optimizer = optim.Adam(pytorch_mlp.parameters(), lr=0.0001)\n",
    "\n",
    "# --- 4. Training Loop ---\n",
    "epochs = 200 # scikit-learn's default is 200, but Adam is fast\n",
    "print(\"\\nTraining PyTorch MLP...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pytorch_mlp.train() # Set model to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = pytorch_mlp(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# --- 5. Evaluate the Model ---\n",
    "pytorch_mlp.eval() # Set model to evaluation mode\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculation for efficiency\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = pytorch_mlp(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1) # Get the index (class) with the highest score\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy_pytorch = accuracy_score(all_labels, all_preds)\n",
    "report_pytorch = classification_report(all_labels, all_preds, target_names=['Under-done', 'Perfect', 'Burnt'])\n",
    "\n",
    "print(f\"\\n--- PyTorch MLP Results ---\")\n",
    "print(f\"Accuracy: {accuracy_pytorch:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report_pytorch)\n",
    "\n",
    "print(f\"\\nCompare to scikit-learn's MLP accuracy: {accuracy_mlp:.4f}\")\n",
    "\n",
    "# --- TODO: Visualize your results: accuracy and runtime ---\n",
    "import matplotlib.pyplot as plt\n",
    "# Example: Bar chart comparing accuracies\n",
    "labels = ['scikit-learn MLP', 'PyTorch MLP']\n",
    "accuracies = [accuracy_mlp, accuracy_pytorch]\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x, accuracies, color=['blue', 'orange'])\n",
    "plt.xticks(x, labels)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('MLP Model Accuracy Comparison')\n",
    "plt.ylim(0, 1)\n",
    "for i, v in enumerate(accuracies):\n",
    "    plt.text(i, v, f\"{v:.4f}\", ha='center')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd76780",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387218f",
   "metadata": {},
   "source": [
    "## **Part 2: Object Detection with YOLO**\n",
    "\n",
    "Now we move from structured data to **unstructured data (images)**.\n",
    "\n",
    "As the lecture explained, MLPs are not good for images. We need [**Convolutional Neural Networks (CNNs)**](https://github.com/vzhou842/cnn-from-scratch). \n",
    "\n",
    "[**YOLO (You Only Look Once)**](https://github.com/ultralytics/ultralytics) is a famous, fast, and accurate CNN-based model for **object detection**.\n",
    "\n",
    "Object detection is harder than classification. Instead of just saying \"This is an image of a cat,\" it says, \"There is a cat at *these* coordinates [x, y, w, h].\"\n",
    "\n",
    "We will use the `ultralytics` library, which makes it very easy to load and use a pre-trained YOLOv8 model.\n",
    "\n",
    "**Your Task:**\n",
    "1.  Load the pre-trained `'yolov8n.pt'` model.\n",
    "2.  Run the model on the `sample_image_urls` provided. One image has many objects which could be classificed into umbrella, sign, car...\n",
    "3.  Display the results.\n",
    "4.  **Find your own image which contain multiple objects of different classes** on the internet, provide the **URL** (not local file path), and run the model on it. Does it work?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cefd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def find_images_with_pathlib(folder_dir: str, recursive: bool = True):\n",
    "    \"\"\"\n",
    "    Finds all image files in a given folder using the pathlib module.\n",
    "\n",
    "    Args:\n",
    "        folder_dir (str): The path to the folder to search.\n",
    "        recursive (bool): If True, searches subfolders as well. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of absolute paths to the found image files.\n",
    "    \"\"\"\n",
    "    image_files = []\n",
    "    \n",
    "    # Define the set of image file extensions to look for.\n",
    "    # Using a set for efficient membership testing.\n",
    "    image_extensions = {\".jpg\", \".jpeg\", \".png\", \".webp\"}\n",
    "\n",
    "    try:\n",
    "        # Create a Path object for the given directory\n",
    "        folder_path = Path(folder_dir)\n",
    "        \n",
    "        # Determine which iterator to use based on the 'recursive' flag\n",
    "        if recursive:\n",
    "            # path.rglob('*') recursively finds all files and directories\n",
    "            path_iterator = folder_path.rglob('*')\n",
    "        else:\n",
    "            # path.glob('*') finds files and directories only in the top level\n",
    "            path_iterator = folder_path.glob('*')\n",
    "\n",
    "        # Loop through all the paths found by the iterator\n",
    "        for file_path in path_iterator:\n",
    "            # Check if the path is a file and its extension is in our set\n",
    "            if file_path.is_file() and file_path.suffix.lower() in image_extensions:\n",
    "                # If it's an image file, add its absolute path to our list\n",
    "                image_files.append(str(file_path.resolve()))\n",
    "                \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The directory '{folder_dir}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "    return image_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c75369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 1. Load the pre-trained YOLOv8n model (n = 'nano', the smallest version)\n",
    "print(\"Loading YOLOv8n model...\")\n",
    "yolo_model = YOLO('yolov8n.pt')\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# 2. Define sample image URLs\n",
    "sample_image_urls = find_images_with_pathlib(\"sample_images\", recursive=True)\n",
    "\n",
    "# 3. Run prediction on the sample images\n",
    "for url in sample_image_urls:\n",
    "    print(f\"\\n--- Running prediction on: {url} ---\")\n",
    "    # Run prediction directly from the URL\n",
    "    results = yolo_model.predict(url)\n",
    "    \n",
    "    # Get the first result object\n",
    "    res = results[0]\n",
    "    \n",
    "    # Print what it found\n",
    "    print(\"Detections:\")\n",
    "    for box in res.boxes:\n",
    "        class_id = int(box.cls)\n",
    "        class_name = yolo_model.names[class_id]\n",
    "        confidence = float(box.conf)\n",
    "        print(f\"  - Found '{class_name}' with {confidence*100:.2f}% confidence.\")\n",
    "\n",
    "    # Plot the image with bounding boxes\n",
    "    # This creates a numpy array of the image with boxes drawn on it\n",
    "    res_plotted = res.plot()\n",
    "    \n",
    "    # Display the image\n",
    "    # Convert BGR (from OpenCV) to RGB (for Matplotlib)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(res_plotted[..., ::-1]) \n",
    "    plt.axis('off')\n",
    "    plt.title(f\"YOLOv8 Detections on {url.split('/')[-1]}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce55c62",
   "metadata": {},
   "source": [
    "#### Task 2.1: Test YOLO on Your Own Image\n",
    "\n",
    "Go to Google Images, find picture of some objects, copy the image address, and paste it below. See if YOLO can find it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: Test YOLOv8n on your own images, and visualize them ---\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c939de7",
   "metadata": {},
   "source": [
    "#### Task 2.2: Webcam Inference\n",
    "\n",
    "Create a real-time food classification system using your webcam. For the way to use Webcam as an input, please check opencv document [OpenCV: Get Started with Videos](https://docs.opencv.org/4.x/dd/d43/tutorial_py_video_display.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacbd2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: Test YOLOv8n with your webcam, and visualize the results ---\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19b99b0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22502fe",
   "metadata": {},
   "source": [
    "## **Part 3: Final Challenge - Food Image Classification CNN**\n",
    "\n",
    "### Overview\n",
    "In this final challenge, we will build a **CNN Classifier** from scratch to classify food images. We'll use the **Food-101 dataset** (a public dataset with 101 food categories) and demonstrate:\n",
    "\n",
    "1. **CNN Architecture Design** - A custom CNN optimized for food classification\n",
    "2. **Data Pipeline** - Image loading, augmentation, and preprocessing\n",
    "3. **Training & Validation** - Full training loop with early stopping and model checkpointing\n",
    "4. **Evaluation & Inference** - Performance metrics and visualization\n",
    "5. **Optimization Techniques** - Data augmentation, learning rate scheduling, and regularization\n",
    "\n",
    "### Dataset Information\n",
    "- **Food\\-101 Dataset**: 101 food categories with ~1000 images per class\n",
    "- **Total Images**: ~101,000 images (split: 75,750 training, 25,250 test)\n",
    "- **Image Size**: Typically 512x512 or can be resized to 224x224 for efficiency\n",
    "- **Access**: Can be downloaded via `torchvision` or Kaggle\n",
    "\n",
    "For this exercise, we'll work with a **subset of the dataset** to ensure it runs on local CPU/GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3232a57",
   "metadata": {},
   "source": [
    "### **Step 3.1: Dataset Setup - Food-101 Dataset**\n",
    "\n",
    "**âš ï¸Attentionâš ï¸:** You will need at least 15 GB storage space to run through the whole process. If you don't have enough space on your machine, please connect an external drive and set the MACRO `DATA_DIR` to your own dataset location.\n",
    "\n",
    "**1.  Set up File configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Dataset configuration. For Google Colab users, you may find it at Google Drive, mounted as /content/drive/MyDrive/ etc.\n",
    "DATA_DIR = \"./data\"\n",
    "DATASET_NAME = \"Food-101\"\n",
    "IMG_SIZE = 224  # Resizing all images to 224x224 for efficiency\n",
    "NUM_CLASSES = 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a733a3bf",
   "metadata": {},
   "source": [
    "**2.  Define data transforms (preprocessing and augmentation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e549fd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training transforms: includes augmentation\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.2),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet normalization\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Test/Validation transforms: no augmentation\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd011373",
   "metadata": {},
   "source": [
    "**3.  Prepare Dataset and DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "snjl1naf0xn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Downloading Food-101 dataset... (This may take a few minutes on first run)\")\n",
    "print(\"Dataset size: ~4-5 GB\")\n",
    "\n",
    "# Download and load Food-101 dataset\n",
    "print(\"Loading training data...\")\n",
    "train_dataset = datasets.Food101(\n",
    "    root=DATA_DIR, \n",
    "    split='train', \n",
    "    transform=train_transforms,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "print(\"Loading test data...\")\n",
    "test_dataset = datasets.Food101(\n",
    "    root=DATA_DIR, \n",
    "    split='test', \n",
    "    transform=test_transforms,\n",
    "    download=False\n",
    ")\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
    "print(f\"Classes: {train_dataset.classes[:10]}... (showing first 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fd5a4",
   "metadata": {},
   "source": [
    "**Attention:** if you have access to a device with GPU (CUDA installed), you may proceed with the full dataset of Food-101 for training. Otherwise, please estimate training time here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9333e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nEstimated training time:\")\n",
    "\n",
    "if str(device) == 'cpu':\n",
    "    est_sec_per_batch = 0.5  # Rough estimate for CPU\n",
    "    est_time_per_epoch = len(train_loader) * est_sec_per_batch\n",
    "    est_total_time_30_epochs = est_time_per_epoch * 30\n",
    "\n",
    "    print(f\"  Device: CPU\")\n",
    "    print(f\"  Per batch: ~{est_sec_per_batch:.1f} sec\")\n",
    "    print(f\"  Per epoch: ~{est_time_per_epoch:.0f} sec (~{est_time_per_epoch/60:.1f} min)\")\n",
    "    print(f\"  30 epochs: ~{est_total_time_30_epochs/60:.1f} minutes (~{est_total_time_30_epochs/3600:.1f} hours)\")\n",
    "\n",
    "    if est_total_time_30_epochs > 10800:  # > 3 hours\n",
    "        print(f\"  ðŸ’¡ Tip: Training may take a while on CPU. Consider using fewer epochs\")\n",
    "        print(f\"           or stopping early if accuracy plateaus.\")\n",
    "else:\n",
    "    est_sec_per_batch = 0.1  # Rough estimate for GPU\n",
    "    est_time_per_epoch = len(train_loader) * est_sec_per_batch\n",
    "    est_total_time_30_epochs = est_time_per_epoch * 30\n",
    "\n",
    "    print(f\"  Device: GPU\")\n",
    "    print(f\"  Per batch: ~{est_sec_per_batch:.1f} sec\")\n",
    "    print(f\"  Per epoch: ~{est_time_per_epoch:.0f} sec (~{est_time_per_epoch/60:.1f} min)\")\n",
    "    print(f\"  30 epochs: ~{est_total_time_30_epochs/60:.1f} minutes (~{est_total_time_30_epochs/3600:.1f} hours)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f863ff0",
   "metadata": {},
   "source": [
    "If the training time is too long, e.g. more than 24 hours, we suggest using subsets of Food-101 for a faster process. \n",
    "\n",
    "**Skip two following cells if you plan to use the full dataset:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22bcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Easy subset (3 classes, ~2000 images) - RECOMMENDED FOR CPU\n",
    "SELECTED_CLASSES = ['pizza', 'steak', 'sushi']\n",
    "SUBSET_NAME = \"Easy (3 classes)\"\n",
    "\n",
    "# Option 2: Medium subset (5 classes, ~3000 images)\n",
    "# SELECTED_CLASSES = ['pizza', 'steak', 'sushi', 'chicken_wings', 'caesar_salad']\n",
    "# SUBSET_NAME = \"Medium (5 classes)\"\n",
    "\n",
    "# Option 3: Advanced subset (5 challenging classes)\n",
    "# SELECTED_CLASSES = ['apple_pie', 'cheese_burger', 'french_fries', 'fried_rice', 'ice_cream']\n",
    "# SUBSET_NAME = \"Advanced (5 classes)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ff3512",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nCreating subset with selected classes...\")\n",
    "\n",
    "# Get class name to index mapping\n",
    "class_to_idx = train_dataset.class_to_idx\n",
    "idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "# Get indices for selected classes\n",
    "selected_class_indices = [class_to_idx[cls] for cls in SELECTED_CLASSES]\n",
    "print(f\"  Class indices: {list(zip(SELECTED_CLASSES, selected_class_indices))}\")\n",
    "\n",
    "# Filter training dataset\n",
    "print(\"  Filtering training data...\")\n",
    "train_indices = [i for i, (_, label) in enumerate(train_dataset)\n",
    "                 if label in selected_class_indices]\n",
    "train_dataset = Subset(train_dataset, train_indices)\n",
    "print(f\"  âœ“ Selected {len(train_dataset)} training samples\")\n",
    "\n",
    "# Filter test dataset\n",
    "print(\"  Filtering test data...\")\n",
    "test_indices = [i for i, (_, label) in enumerate(test_dataset)\n",
    "                if label in selected_class_indices]\n",
    "test_dataset = Subset(test_dataset, test_indices)\n",
    "print(f\"  âœ“ Selected {len(test_dataset)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1700cb1",
   "metadata": {},
   "source": [
    "Load dataset into training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786d8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_TRAIN = True\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=SHUFFLE_TRAIN,\n",
    "    num_workers=4,  # Adjust based on your system (0 on Windows/MacOS without CUDA GPU)\n",
    "    pin_memory=True if str(device) != 'cpu' else False\n",
    ")\n",
    "\n",
    "# --- TODO: Write codes to define a test DataLoader ---\n",
    "test_loader = DataLoader(\n",
    "\n",
    ")\n",
    "# ---------------------------------\n",
    "\n",
    "print(f\"\\nDataLoaders created:\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9mbs26vbjju",
   "metadata": {},
   "source": [
    "### Step 3.2: CNN Architecture Design: A baseline model\n",
    "\n",
    "Custom CNN Architecture for Food Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rospnbx2o9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom Convolutional Neural Network for Food-101 classification.\n",
    "    \n",
    "    Architecture:\n",
    "    - 4 convolutional blocks with batch normalization and dropout\n",
    "    - Progressive increase in filters: 32 -> 64 -> 128 -> 256\n",
    "    - Max pooling after each conv block\n",
    "    - 2 fully connected layers with dropout\n",
    "    - Output layer with 101 classes\n",
    "    \n",
    "    Total parameters: ~3.2M (manageable for training on CPU/GPU)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=101, dropout_rate=0.5):\n",
    "        super(FoodCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional Block 1: Input (3 channels) -> 32 filters\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 224 -> 112\n",
    "            nn.Dropout(dropout_rate * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Convolutional Block 2: 32 -> 64 filters\n",
    "        # --- TODO: Write codes to define a Convolutional Block: 32 -> 64 filters ---\n",
    "        self.conv2 = nn.Sequential(\n",
    "\n",
    "        )\n",
    "        # ---------------------------------\n",
    "        \n",
    "        # Convolutional Block 3: 64 -> 128 filters\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 56 -> 28\n",
    "            nn.Dropout(dropout_rate * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Convolutional Block 4: 128 -> 256 filters\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 28 -> 14\n",
    "            nn.Dropout(dropout_rate * 0.3)\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling: 14x14 -> 1x1\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully Connected Layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(dropout_rate * 0.7),\n",
    "            \n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Conv blocks\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4zhhsxi3t7p",
   "metadata": {},
   "source": [
    "### Step 3.3: Training Loop Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "print(\"Initializing FoodCNN (custom architecture)...\")\n",
    "model = FoodCNN(num_classes=NUM_CLASSES, dropout_rate=0.5).to(device)\n",
    "\n",
    "# Print model summary\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "total_params = count_parameters(model)\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n",
    "print(f\"Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4189d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, mode='Validation'):\n",
    "    \"\"\"Evaluate model on a given dataset.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(data_loader, desc=f\"{mode}\", leave=False):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Accumulate metrics\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abr4w55vj6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Training configuration\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 15\n",
    "PATIENCE = 5  # Early stopping patience\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Learning rate scheduler: reduce LR when validation loss plateaus\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_acc': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "best_model_path = \"./best_food_model.pth\"\n",
    "\n",
    "# Training Loop\n",
    "print(\"Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Total epochs: {EPOCHS}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start = time.time()\n",
    "    \n",
    "    # ==================== Training Phase ====================\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch [{epoch+1}/{EPOCHS}] Training', leave=True)\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate metrics\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_loss = train_loss / train_total\n",
    "        current_acc = 100 * train_correct / train_total\n",
    "        progress_bar.set_postfix({'Loss': f'{current_loss:.4f}', 'Acc': f'{current_acc:.2f}%'})\n",
    "    \n",
    "    train_loss = train_loss / train_total\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    \n",
    "    # ==================== Validation Phase ====================\n",
    "    val_loss, val_acc, _, _ = evaluate_model(model, test_loader, criterion, mode='Validation')\n",
    "    \n",
    "    # Store history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['learning_rates'].append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "    # ==================== Early Stopping & Model Checkpointing ====================\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        improvement = \"âœ“ (Best)\"\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        improvement = \"\"\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1:3d}/{EPOCHS}] | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}% {improvement} | \"\n",
    "          f\"Time: {epoch_time:.1f}s\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if patience_counter >= PATIENCE:\n",
    "        print(f\"\\nEarly stopping triggered after {PATIENCE} epochs without improvement.\")\n",
    "        break\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining completed in {total_time/60:.1f} minutes\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")\n",
    "\n",
    "# Load best model for final evaluation\n",
    "print(f\"\\nLoading best model from checkpoint...\")\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(\"Best model loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bjae38gujn",
   "metadata": {},
   "source": [
    "### Step 3.4: Model Evaluation and Visualization\n",
    "\n",
    "**1. Evaluate the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rpw74suvnkk",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get final predictions on test set\n",
    "final_val_loss, final_val_acc, test_preds, test_labels = evaluate_model(\n",
    "    model, test_loader, criterion, mode='Final Evaluation'\n",
    ")\n",
    "\n",
    "# Calculate additional metrics\n",
    "precision = precision_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "recall = recall_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "f1 = f1_score(test_labels, test_preds, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\nTest Set Performance:\")\n",
    "print(f\"  Accuracy:  {final_val_acc:.2f}%\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "print(f\"  Loss:      {final_val_loss:.4f}\")\n",
    "\n",
    "# ==================== Training History Visualization ====================\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('Training History and Performance Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "axes[0, 0].plot(history['train_loss'], label='Training Loss', linewidth=2)\n",
    "axes[0, 0].plot(history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Loss Over Epochs')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy curves\n",
    "axes[0, 1].plot(history['train_acc'], label='Training Accuracy', linewidth=2)\n",
    "axes[0, 1].plot(history['val_acc'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy (%)')\n",
    "axes[0, 1].set_title('Accuracy Over Epochs')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Learning rate schedule\n",
    "axes[1, 0].plot(history['learning_rates'], linewidth=2, color='green')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_title('Learning Rate Schedule')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Metrics summary\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "values = [final_val_acc, precision*100, recall*100, f1*100]\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#f39c12']\n",
    "bars = axes[1, 1].bar(metrics, values, color=colors, alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "axes[1, 1].set_ylabel('Score')\n",
    "axes[1, 1].set_title('Final Test Metrics')\n",
    "axes[1, 1].set_ylim([0, 105])\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, value in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTraining history plot saved as 'training_history.png'\")\n",
    "\n",
    "# ==================== Per-Class Performance ====================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS PERFORMANCE (Top 10 Classes)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "class_names = train_dataset.classes\n",
    "report = classification_report(test_labels, test_preds, target_names=class_names, \n",
    "                               digits=4, zero_division=0, output_dict=True)\n",
    "\n",
    "# Sort by F1 score\n",
    "sorted_classes = sorted([(name, report[name]['f1-score']) for name in class_names],\n",
    "                        key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nTop 10 Best Performing Classes:\")\n",
    "for i, (class_name, f1) in enumerate(sorted_classes[:10], 1):\n",
    "    precision_c = report[class_name]['precision']\n",
    "    recall_c = report[class_name]['recall']\n",
    "    print(f\"{i:2d}. {class_name:20s} - F1: {f1:.4f}, Precision: {precision_c:.4f}, Recall: {recall_c:.4f}\")\n",
    "\n",
    "print(f\"\\nTop 10 Worst Performing Classes:\")\n",
    "for i, (class_name, f1) in enumerate(sorted_classes[-10:], 1):\n",
    "    precision_c = report[class_name]['precision']\n",
    "    recall_c = report[class_name]['recall']\n",
    "    print(f\"{i:2d}. {class_name:20s} - F1: {f1:.4f}, Precision: {precision_c:.4f}, Recall: {recall_c:.4f}\")\n",
    "\n",
    "# ==================== Inference on Sample Images ====================\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INFERENCE ON SAMPLE IMAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get a few test samples\n",
    "model.eval()\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Sample Predictions on Test Images', fontsize=16, fontweight='bold')\n",
    "\n",
    "sample_loader = DataLoader(test_dataset, batch_size=6, shuffle=True)\n",
    "images_batch, labels_batch = next(iter(sample_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images_batch.to(device))\n",
    "    _, predictions = torch.max(outputs, 1)\n",
    "\n",
    "# Denormalize images for display\n",
    "inverse_transform = transforms.Compose([\n",
    "    transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "for idx, (ax, image, true_label, pred_label) in enumerate(\n",
    "    zip(axes.flat, images_batch, labels_batch, predictions)):\n",
    "    \n",
    "    # Denormalize and display\n",
    "    image_np = inverse_transform(image).clamp(0, 1).numpy().transpose(1, 2, 0)\n",
    "    ax.imshow(image_np)\n",
    "    \n",
    "    true_class = class_names[true_label.item()]\n",
    "    pred_class = class_names[pred_label.item()]\n",
    "    \n",
    "    # Color code based on correctness\n",
    "    if true_label.item() == pred_label.item():\n",
    "        ax.set_title(f'True: {true_class}\\nPred: {pred_class}', color='green', fontweight='bold')\n",
    "    else:\n",
    "        ax.set_title(f'True: {true_class}\\nPred: {pred_class}', color='red', fontweight='bold')\n",
    "    \n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample predictions plot saved as 'sample_predictions.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc120947",
   "metadata": {},
   "source": [
    "**2. Your task:**\n",
    "\n",
    "*  Find a picture from Google and download it.\n",
    "*  Use the model you've just trained and evaluated, to predict the single image downloaded from Google.\n",
    "*  Visualize the prediction, and print statistical results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: Single Image Prediction ---\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945a8b61",
   "metadata": {},
   "source": [
    "**3. Summary of baseline model:** Phase I report\n",
    "\n",
    "Please fill in the information required based on your baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3afa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_template = \"\"\"\n",
    "================================================================================\n",
    "PART 3 FINAL CHALLENGE - Phase I \n",
    "================================================================================\n",
    "\n",
    "Step 3.1 to 3.4\n",
    "\n",
    "================================================================================\n",
    "SECTION 1: PROBLEM STATEMENT & APPROACH\n",
    "================================================================================\n",
    "\n",
    "1.1 Problem Statement\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Clearly state what problem you're solving:\n",
    "  â€¢ Dataset: Food-101 (101 food categories, ~101k images, or subset)\n",
    "  â€¢ Objective: [Your objective]\n",
    "  â€¢ Performance Goal: [Target accuracy]\n",
    "  â€¢ Constraints: [Time, memory, etc.]\n",
    "\n",
    "Example:\n",
    "  \"Build a CNN to classify food images into 101 categories with >80% accuracy\n",
    "   using efficient training on GPU with early stopping.\"\n",
    "\n",
    "\n",
    "1.2 Approach\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Describe baseline approach (step 3.3):\n",
    "  â€¢ Model architecture: [Type and complexity]\n",
    "  â€¢ Training strategy: [Methods used]\n",
    "  â€¢ Data handling: [Augmentation, preprocessing]\n",
    "  â€¢ Evaluation: [Metrics]\n",
    "\n",
    "Example:\n",
    "  \"We started with a custom CNN with 4 conv blocks (32â†’64â†’128â†’256 filters),\n",
    "   trained with Adam optimizer, basic augmentation, and early stopping.\"\n",
    "\n",
    "\n",
    "================================================================================\n",
    "SECTION 2: IMPLEMENTATION DETAILS\n",
    "================================================================================\n",
    "\n",
    "2.1 Model Architecture\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Describe the baseline model in step 3.2:\n",
    "\n",
    "a) Architecture Choice: [Custom CNN / Transfer Learning / Hybrid]\n",
    "   \n",
    "   If Custom CNN:\n",
    "     â€¢ Number of layers: [N]\n",
    "     â€¢ Filter progression: [Values]\n",
    "     â€¢ Total parameters: [Number]\n",
    "     â€¢ Key components: [BN, dropout, pooling, etc.]\n",
    "   \n",
    "   If Transfer Learning:\n",
    "     â€¢ Base model: [ResNet18/ResNet50/EfficientNet/etc.]\n",
    "     â€¢ Frozen layers: [Which ones]\n",
    "     â€¢ Fine-tuned layers: [Which ones]\n",
    "     â€¢ Custom head: [Architecture]\n",
    "   \n",
    "   Example:\n",
    "     \"Custom CNN with 4 convolutional blocks with batch normalization.\n",
    "      Progressive filter expansion: 32â†’64â†’128â†’256.\n",
    "      Global average pooling to reduce dimensions.\n",
    "      Two fully connected layers (512, 256) with dropout.\n",
    "      Total: 3.2M parameters.\"\n",
    "\n",
    "\n",
    "2.2 Data Pipeline\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Describe your data handling:\n",
    "\n",
    "a) Data Augmentation:\n",
    "   â€¢ Techniques used: [List techniques]\n",
    "   â€¢ Why: [Justification]\n",
    "   â€¢ Effect on training: [Observed impact]\n",
    "\n",
    "   Example:\n",
    "     \"Applied random horizontal/vertical flips, rotation (15Â°),\n",
    "      color jitter, and affine transformations.\n",
    "      Increased generalization and reduced overfitting by ~5%.\"\n",
    "\n",
    "b) Data Preprocessing:\n",
    "   â€¢ Image size: [Resolution]\n",
    "   â€¢ Normalization: [Method]\n",
    "   â€¢ Train/test split: [Ratio]\n",
    "   â€¢ Batch size: [Size]\n",
    "\n",
    "   Example:\n",
    "     \"Resized all images to 224Ã—224 (from 512Ã—512).\n",
    "      Normalized using ImageNet statistics (mean, std).\n",
    "      Used 75/25 train/test split (75,750 / 25,250 samples).\n",
    "      Batch size: 32 (balance between speed and memory).\"\n",
    "\n",
    "\n",
    "2.3 Training Configuration\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Document your training setup:\n",
    "\n",
    "a) Hyperparameters:\n",
    "   â€¢ Learning rate: [Initial value]\n",
    "   â€¢ Optimizer: [Type and settings]\n",
    "   â€¢ Loss function: [Type]\n",
    "   â€¢ Batch size: [Size]\n",
    "   â€¢ Epochs: [Number]\n",
    "\n",
    "   Example:\n",
    "     \"Adam optimizer with LR=0.001, no weight decay.\n",
    "      CrossEntropyLoss for multi-class classification.\n",
    "      Batch size: 32, max epochs: 30.\"\n",
    "\n",
    "b) Regularization:\n",
    "   â€¢ Dropout: [Values]\n",
    "   â€¢ Weight decay: [Value]\n",
    "   â€¢ Early stopping: [Patience]\n",
    "   â€¢ Others: [List]\n",
    "\n",
    "   Example:\n",
    "     \"Dropout: 0.5 in FC layers, 0.15 in conv blocks.\n",
    "      Weight decay: 1e-4 (L2 regularization).\n",
    "      Early stopping patience: 5 epochs.\n",
    "      Gradient clipping: max_norm=1.0.\"\n",
    "\n",
    "c) Learning Rate Schedule:\n",
    "   â€¢ Type: [ReduceLROnPlateau / StepLR / Warmup / etc.]\n",
    "   â€¢ Parameters: [Values]\n",
    "\n",
    "   Example:\n",
    "     \"Used ReduceLROnPlateau: reduce LR by 0.5x when loss plateaus.\n",
    "      Patience: 3 epochs, minimum LR: 1e-6.\"\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(report_template)\n",
    "\n",
    "# Save template\n",
    "with open(\"FINAL_REPORT_1.txt\", 'w') as f:\n",
    "    f.write(report_template)\n",
    "\n",
    "print(\"\\nâœ“ Template saved to 'FINAL_REPORT_1.txt'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4642e61e",
   "metadata": {},
   "source": [
    "### Part 3.5: Optimization (Optional)\n",
    "\n",
    "We only provide hints about possible optimizations. You may try with your own way of fine-tuning hyperparameters, optimizing baseline models, or totally design a new model. It is ok to develop and compare your work with another baseline model, e.g. Vision Transformer.\n",
    "\n",
    "**1.  Try `Transfer-Learning` with ResNet18 to accelerate the training process:** Transfer learning leverages knowledge from large-scale datasets (e.g., ImageNet)\n",
    "to improve performance on smaller datasets.\n",
    "\n",
    "ResNet18 already learned (from 1.2M ImageNet images):\n",
    "* Layer 1: Edges, colors, basic patterns\n",
    "* Layer 2: Textures and simple shapes\n",
    "* Layer 3: Complex object parts\n",
    "* Layer 4: Object categories (dogs, cars, trees, etc.)\n",
    "\n",
    "Advantages:\n",
    "* Significantly faster training (10x faster)\n",
    "* Better accuracy (especially with limited data)\n",
    "* Requires fewer epochs\n",
    "* Less prone to overfitting\n",
    "\n",
    "Disadvantages:\n",
    "* Less interpretability\n",
    "* Requires careful fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodCNNTransfer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transfer Learning approach using pre-trained ResNet18.\n",
    "    Much faster to train than training with the baseline.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes=101, pretrained=True):\n",
    "        super(FoodCNNTransfer, self).__init__()\n",
    "        \n",
    "        # Load pre-trained ResNet18\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=pretrained)\n",
    "        \n",
    "        # Freeze early layers to preserve learned features\n",
    "        for param in self.resnet.layer1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in self.resnet.layer2.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Replace the final fully connected layer\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d0db56",
   "metadata": {},
   "source": [
    "**2. Ensemble Methods**: Ensemble methods combine predictions from multiple models to achieve better accuracy and robustness.\n",
    "\n",
    "Types:\n",
    "*  Model Averaging: Combine predictions from multiple trained models\n",
    "*  Voting: Majority voting among multiple models\n",
    "*  Stacking: Train a meta-model on predictions from multiple models\n",
    "*  Boosting: Sequential training with focus on misclassified samples\n",
    "\n",
    "\n",
    "**3. Hyperparameter Optimization:** possible key hyperparameters to tune:\n",
    "\n",
    "*  Learning Rate: [1e-4, 1e-3, 1e-2]\n",
    "*  Batch Size: [16, 32, 64, 128]\n",
    "*  Dropout Rate: [0.2, 0.3, 0.5, 0.7]\n",
    "*  Weight Decay: [0, 1e-5, 1e-4, 1e-3]\n",
    "*  Optimizer: [Adam, SGD, RMSprop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6285b1",
   "metadata": {},
   "source": [
    "**4. Vision Transformers (ViT)**: Vision Transformers apply the Transformer architecture to image tasks.\n",
    "\n",
    "Advantages:\n",
    "*  State-of-the-art performance\n",
    "*  Better with large datasets\n",
    "*  Efficient for high-resolution images\n",
    "*  Good transfer learning properties\n",
    "\n",
    "Disadvantages:\n",
    "*  Requires more data for training from scratch\n",
    "*  More computationally expensive\n",
    "*  Slower inference\n",
    "\n",
    "Pre-trained options:\n",
    "*  ViT-Base: 86M parameters\n",
    "*  ViT-Large: 304M parameters\n",
    "*  DeiT: Distilled ViT (faster inference)\n",
    "\n",
    "**Basic ViT Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6a2967",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "class FoodViT(nn.Module):\n",
    "    def __init__(self, num_classes=101, pretrained=True):\n",
    "        super(FoodViT, self).__init__()\n",
    "        \n",
    "        # Load pre-trained Vision Transformer\n",
    "        if pretrained:\n",
    "            self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            self.vit = vit_b_16(weights=None)\n",
    "        \n",
    "        # Replace classifier head\n",
    "        in_features = self.vit.heads.head.in_features\n",
    "        self.vit.heads.head = nn.Linear(in_features, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.vit(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edf733",
   "metadata": {},
   "source": [
    "**ViT-Specific Data Transformations:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4616fdc3",
   "metadata": {},
   "source": [
    "#### Now it's your turn! **Your Goal:** Get the **highest Test Accuracy** you can. This is a tough dataset, so >75% is a great start. >90% is excellent. Subset is good if you don't have access to GPU devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1262989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- TODO: Your new model: dataloader, model design, training loop, validation process ---\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca644a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_template = \"\"\"\n",
    "================================================================================\n",
    "PART 3 FINAL CHALLENGE - Phase II\n",
    "================================================================================\n",
    "\n",
    "Step 3.5\n",
    "\n",
    "================================================================================\n",
    "SECTION 1: OPTIMIZATION TECHNIQUES APPLIED\n",
    "================================================================================\n",
    "\n",
    "1.1 Data Augmentation Optimizations\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Which augmentation techniques did you use?\n",
    "  â–¡ Basic (flips, rotation, jitter)\n",
    "  â–¡ Mixup\n",
    "  â–¡ CutMix\n",
    "  â–¡ RandAugment\n",
    "  â–¡ AutoAugment\n",
    "  â–¡ Other: ___________\n",
    "\n",
    "Results:\n",
    "  â€¢ Before: [Accuracy]\n",
    "  â€¢ After: [Accuracy]\n",
    "  â€¢ Improvement: [+X%]\n",
    "\n",
    "Example:\n",
    "  \"âœ“ RandAugment applied (2 operations, magnitude 9)\n",
    "   Before: 76%  â†’  After: 79%  (+3% improvement)\"\n",
    "\n",
    "\n",
    "1.2 Model Architecture Optimizations\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Which architectural improvements did you implement?\n",
    "  â–¡ Standard CNN (baseline)\n",
    "  â–¡ Residual connections (skip connections)\n",
    "  â–¡ Depthwise separable convolutions\n",
    "  â–¡ Attention mechanisms (SE-blocks)\n",
    "  â–¡ Transfer learning\n",
    "  â–¡ Other: ___________\n",
    "\n",
    "Results:\n",
    "  â€¢ Architecture: [Description]\n",
    "  â€¢ Parameters: [Count]\n",
    "  â€¢ Speed: [Training time]\n",
    "  â€¢ Accuracy: [Value]\n",
    "\n",
    "Example:\n",
    "  \"âœ“ Residual connections added for gradient flow\n",
    "   Parameters: 3.4M (baseline 3.2M)\n",
    "   Training time: +10% per epoch\n",
    "   Accuracy improvement: +2-3%\"\n",
    "\n",
    "\n",
    "1.3 Data Loading Optimizations\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Which data pipeline improvements did you use?\n",
    "  â–¡ Standard DataLoader\n",
    "  â–¡ Weighted random sampling\n",
    "  â–¡ Gradient accumulation\n",
    "  â–¡ Prefetch & pin memory\n",
    "  â–¡ Other: ___________\n",
    "\n",
    "Results:\n",
    "  â€¢ Technique: [Description]\n",
    "  â€¢ Effect: [Impact]\n",
    "  â€¢ Benefit: [Speed/accuracy/stability]\n",
    "\n",
    "Example:\n",
    "  \"âœ“ Pin memory enabled for faster GPU transfer\n",
    "   âœ“ 4 num_workers for parallel data loading\n",
    "   Effect: +15% training speed with no accuracy loss\"\n",
    "\n",
    "\n",
    "1.4 Training Optimizations\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Which training improvements did you implement?\n",
    "  â–¡ Standard training\n",
    "  â–¡ Learning rate warmup\n",
    "  â–¡ Label smoothing\n",
    "  â–¡ Stochastic weight averaging\n",
    "  â–¡ Other: ___________\n",
    "\n",
    "Results:\n",
    "  â€¢ Technique: [Description]\n",
    "  â€¢ Epochs saved: [Number]\n",
    "  â€¢ Accuracy gain: [Value]\n",
    "\n",
    "Example:\n",
    "  \"âœ“ Learning rate warmup (5 epochs)\n",
    "   Convergence time: reduced by 8 epochs\n",
    "   âœ“ Label smoothing (smoothing=0.1)\n",
    "   Accuracy: +1-2% improvement\"\n",
    "\n",
    "================================================================================\n",
    "SECTION 2: IMPLEMENTATION DETAILS\n",
    "================================================================================\n",
    "\n",
    "2.1 Model Architecture\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Describe your final model:\n",
    "\n",
    "a) Architecture Choice: [Custom CNN / Transfer Learning / Hybrid]\n",
    "   \n",
    "   If Custom CNN:\n",
    "     â€¢ Number of layers: [N]\n",
    "     â€¢ Filter progression: [Values]\n",
    "     â€¢ Total parameters: [Number]\n",
    "     â€¢ Key components: [BN, dropout, pooling, etc.]\n",
    "   \n",
    "   If Transfer Learning:\n",
    "     â€¢ Base model: [ResNet18/ResNet50/EfficientNet/etc.]\n",
    "     â€¢ Frozen layers: [Which ones]\n",
    "     â€¢ Fine-tuned layers: [Which ones]\n",
    "     â€¢ Custom head: [Architecture]\n",
    "   \n",
    "   Example:\n",
    "     \"Custom CNN with 4 convolutional blocks with batch normalization.\n",
    "      Progressive filter expansion: 32â†’64â†’128â†’256.\n",
    "      Global average pooling to reduce dimensions.\n",
    "      Two fully connected layers (512, 256) with dropout.\n",
    "      Total: 3.2M parameters.\"\n",
    "\n",
    "b) Key Design Decisions:\n",
    "   â€¢ Why did you choose this architecture?\n",
    "   â€¢ How does it compare to alternatives?\n",
    "   â€¢ What problems does it solve?\n",
    "\n",
    "   Example:\n",
    "     \"Chose custom CNN to understand architectural principles.\n",
    "      Added batch normalization for training stability.\n",
    "      Used dropout to prevent overfitting on 101 classes.\n",
    "      Global avg pooling more efficient than flatten.\"\n",
    "\n",
    "\n",
    "2.2 Data Pipeline\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Describe your data handling:\n",
    "\n",
    "a) Data Augmentation:\n",
    "   â€¢ Techniques used: [List techniques]\n",
    "   â€¢ Why: [Justification]\n",
    "   â€¢ Effect on training: [Observed impact]\n",
    "\n",
    "   Example:\n",
    "     \"Applied random horizontal/vertical flips, rotation (15Â°),\n",
    "      color jitter, and affine transformations.\n",
    "      Increased generalization and reduced overfitting by ~5%.\"\n",
    "\n",
    "b) Data Preprocessing:\n",
    "   â€¢ Image size: [Resolution]\n",
    "   â€¢ Normalization: [Method]\n",
    "   â€¢ Train/test split: [Ratio]\n",
    "   â€¢ Batch size: [Size]\n",
    "\n",
    "   Example:\n",
    "     \"Resized all images to 224Ã—224 (from 512Ã—512).\n",
    "      Normalized using ImageNet statistics (mean, std).\n",
    "      Used 75/25 train/test split (75,750 / 25,250 samples).\n",
    "      Batch size: 32 (balance between speed and memory).\"\n",
    "\n",
    "\n",
    "2.3 Training Configuration\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Document your training setup:\n",
    "\n",
    "a) Hyperparameters:\n",
    "   â€¢ Learning rate: [Initial value]\n",
    "   â€¢ Optimizer: [Type and settings]\n",
    "   â€¢ Loss function: [Type]\n",
    "   â€¢ Batch size: [Size]\n",
    "   â€¢ Epochs: [Number]\n",
    "\n",
    "   Example:\n",
    "     \"Adam optimizer with LR=0.001, no weight decay.\n",
    "      CrossEntropyLoss for multi-class classification.\n",
    "      Batch size: 32, max epochs: 30.\"\n",
    "\n",
    "b) Regularization:\n",
    "   â€¢ Dropout: [Values]\n",
    "   â€¢ Weight decay: [Value]\n",
    "   â€¢ Early stopping: [Patience]\n",
    "   â€¢ Others: [List]\n",
    "\n",
    "   Example:\n",
    "     \"Dropout: 0.5 in FC layers, 0.15 in conv blocks.\n",
    "      Weight decay: 1e-4 (L2 regularization).\n",
    "      Early stopping patience: 5 epochs.\n",
    "      Gradient clipping: max_norm=1.0.\"\n",
    "\n",
    "c) Learning Rate Schedule:\n",
    "   â€¢ Type: [ReduceLROnPlateau / StepLR / Warmup / etc.]\n",
    "   â€¢ Parameters: [Values]\n",
    "\n",
    "   Example:\n",
    "     \"Used ReduceLROnPlateau: reduce LR by 0.5x when loss plateaus.\n",
    "      Patience: 3 epochs, minimum LR: 1e-6.\"\n",
    "\n",
    "================================================================================\n",
    "SECTION 3: RESULTS & PERFORMANCE\n",
    "================================================================================\n",
    "\n",
    "3.1 Final Results\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Report your final metrics:\n",
    "\n",
    "Test Set Performance:\n",
    "  â€¢ Accuracy: [X.XX%]\n",
    "  â€¢ Precision: [X.XXXX]\n",
    "  â€¢ Recall: [X.XXXX]\n",
    "  â€¢ F1-Score: [X.XXXX]\n",
    "  â€¢ Loss: [X.XXXX]\n",
    "\n",
    "Example:\n",
    "  â€¢ Accuracy: 80.32%\n",
    "  â€¢ Precision: 0.8015\n",
    "  â€¢ Recall: 0.8032\n",
    "  â€¢ F1-Score: 0.8021\n",
    "  â€¢ Loss: 0.6842\n",
    "\n",
    "\n",
    "3.2 Training Progress\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Describe how training evolved:\n",
    "\n",
    "Epoch-wise breakdown:\n",
    "  â€¢ Early epochs (1-5): [Observations]\n",
    "  â€¢ Mid epochs (6-15): [Observations]\n",
    "  â€¢ Late epochs (16+): [Observations]\n",
    "\n",
    "Example:\n",
    "  \"Early: Loss decreased rapidly, good gradient flow\n",
    "   Mid: Validation accuracy plateaued around 78%\n",
    "   Late: Fine-tuning phase, +2% improvement with warmup\"\n",
    "\n",
    "\n",
    "3.3 Comparison to Baselines\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "How does your final model compare?\n",
    "\n",
    "| Model | Accuracy | Parameters | Time | Complexity |\n",
    "|-------|----------|-----------|------|-----------|\n",
    "| Baseline CNN | X% | XM | Xh | Low |\n",
    "| Your Final | Y% | YM | Yh | Medium |\n",
    "| Transfer Learning (if tried) | Z% | ZM | Zh | Low |\n",
    "\n",
    "Example:\n",
    "| Baseline CNN | 75% | 3.2M | 1.0h | Low |\n",
    "| Final Model | 80% | 3.4M | 1.1h | Medium |\n",
    "| ResNet18 (bonus) | 86% | 11.2M | 0.3h | Low |\n",
    "\n",
    "\n",
    "3.4 Per-Class Analysis\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Top and bottom performing classes:\n",
    "\n",
    "Best performing classes:\n",
    "  1. [Class name]: [Accuracy]\n",
    "  2. [Class name]: [Accuracy]\n",
    "  3. [Class name]: [Accuracy]\n",
    "\n",
    "Worst performing classes:\n",
    "  1. [Class name]: [Accuracy]\n",
    "  2. [Class name]: [Accuracy]\n",
    "  3. [Class name]: [Accuracy]\n",
    "\n",
    "Why?\n",
    "  â€¢ [Analysis of good performance]\n",
    "  â€¢ [Analysis of poor performance]\n",
    "\n",
    "Example:\n",
    "Best:\n",
    "  1. Apple pie: 94% (distinct appearance)\n",
    "  2. Donut: 91% (varied presentations)\n",
    "  3. Burger: 89% (consistent structure)\n",
    "\n",
    "Worst:\n",
    "  1. Falafel: 65% (similar to other fried foods)\n",
    "  2. Cannoli: 68% (confusion with similar pastries)\n",
    "  3. Huevos rancheros: 71% (variant forms)\n",
    "\n",
    "Why: Similar-looking foods and variant presentations cause confusion.\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(report_template)\n",
    "\n",
    "# Save template\n",
    "with open(\"FINAL_REPORT_2.txt\", 'w') as f:\n",
    "    f.write(report_template)\n",
    "\n",
    "print(\"\\nâœ“ Template saved to 'FINAL_REPORT_2.txt'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58069730",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_template = \"\"\"\n",
    "================================================================================\n",
    "PART 3 FINAL CHALLENGE - Phase III\n",
    "================================================================================\n",
    "\n",
    "INSIGHTS: What Worked Well?\n",
    "================================================================================\n",
    "\n",
    "Your answer:\n",
    "\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Examples:\n",
    "Three techniques had the most impact:\n",
    "\n",
    "1. Batch Normalization (Critical)\n",
    "   â€¢ Removing BN dropped accuracy by 12%!\n",
    "   â€¢ Stabilized training and enabled deeper networks\n",
    "   â€¢ Should be standard in all CNN designs\n",
    "   â€¢ Insight: Modern CNNs are critically dependent on BN\n",
    "\n",
    "2. Learning Rate Scheduling\n",
    "   â€¢ ReduceLROnPlateau adapted to learning dynamics\n",
    "   â€¢ Automatically reduced LR when loss plateaued\n",
    "   â€¢ Prevented overfitting and improved convergence\n",
    "   â€¢ Insight: Adaptive LR > fixed LR\n",
    "\n",
    "3. Data Augmentation (RandAugment)\n",
    "   â€¢ +3% improvement with zero overhead\n",
    "   â€¢ Simple one-line change to code\n",
    "   â€¢ More robust than manual augmentation\n",
    "   â€¢ Insight: Automated augmentation policies are powerful\n",
    "\n",
    "================================================================================\n",
    "\"\"\"\n",
    "\n",
    "print(report_template)\n",
    "\n",
    "# Save template\n",
    "with open(\"FINAL_REPORT_3.txt\", 'w') as f:\n",
    "    f.write(report_template)\n",
    "\n",
    "print(\"\\nâœ“ Template saved to 'FINAL_REPORT_3.txt'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
